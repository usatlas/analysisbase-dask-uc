labextension:
  factory:
    module: dask_kubernetes
    class: KubeCluster
    args: []
    kwargs: {}
  default:
    workers: 3
    adapt:
      minimum: 0
      maximum: 100
  initial: []

distributed:
  dashboard:
    link: "https://n4dask.af.uchicago.edu/{host}/status"
    # link: "{scheme}://{host}:{port}/status"
  scheduler:
    work-stealing: false

kubernetes:
  name: "dask-{user}-{uuid}"
  namespace: af-jupyter
  env: {}
  count:
    start: 1
    max: 100
  image: "ghcr.io/dask/dask:latest"
  resources: {}
  scheduler-service-wait-timeout: 30
  scheduler-service-name-resolution-retries: 20

  scheduler-service-template:
    apiVersion: v1
    kind: Service
    spec:
      selector:
        dask.org/cluster-name: "" # Cluster name will be added automatically
        dask.org/component: scheduler
      ports:
        - name: tcp-comm
          protocol: TCP
          port: 8786
          targetPort: 8786
        - name: http-dashboard
          protocol: TCP
          port: 8787
          targetPort: 8787

  scheduler-pdb-template:
    apiVersion: policy/v1
    kind: PodDisruptionBudget
    spec:
      minAvailable: 1
      selector:
        matchLabels:
          dask.org/cluster-name: "" # Cluster name will be added automatically
          dask.org/component: scheduler

  worker-template-path: null

  scheduler-template: # NOT TAKEN INTO ACCOUNT AT ALL!
    kind: Pod
    metadata:
    spec:
      restartPolicy: Never
      serviceAccountName: jupyter
      containers:
        - name: dask-scheduler
          image: ghcr.io/dask/dask:latest
          args:
            - dask
            - scheduler
            - --port
            - "8786"
            - --bokeh-port
            - "8787"
            - --bokeh-prefix
            - "{host}"
          resources:
            limits:
              cpu: "2.0"
              memory: 2G
            requests:
              cpu: "1.5"
              memory: 2G

  worker-template:
    kind: Pod
    metadata:
    spec:
      restartPolicy: Never
      serviceAccountName: jupyter
      containers:
        - name: dask-worker
          image: ghcr.io/dask/dask:latest
          args:
            - dask
            - worker
            - --nthreads
            - "2"
            # - --no-bokeh
            - --memory-limit
            - 5GB
            - --death-timeout
            - "60"
          resources:
            limits:
              cpu: "2"
              memory: 3G
            requests:
              cpu: "1"
              memory: 2G
