labextension:
  factory:
    module: dask_kubernetes
    class: KubeCluster
    args: []
    kwargs: {}
  default:
    workers: 3
    adapt:
      minimum: 0
      maximum: 1000
  initial: []

distributed:
  dashboard:
    link: "https://n4dask.af.uchicago.edu/{host}/status"
    # link: "{scheme}://{host}:{port}/status"
  scheduler:
    work-stealing: false

kubernetes:
  name: "dask-{user}-{uuid}"
  namespace: af-jupyter
  env: {}
  count:
    start: 1
    max: 100
  image: "ghcr.io/dask/dask:latest"
  resources: {}
  scheduler-service-wait-timeout: 30
  scheduler-service-name-resolution-retries: 20

  scheduler-service-template:
    apiVersion: v1
    kind: Service
    spec:
      selector:
        dask.org/cluster-name: "" # Cluster name will be added automatically
        dask.org/component: scheduler
      ports:
        - name: tcp-comm
          protocol: TCP
          port: 8786
          targetPort: 8786
        - name: http-dashboard
          protocol: TCP
          port: 8787
          targetPort: 8787

  scheduler-pdb-template:
    apiVersion: policy/v1
    kind: PodDisruptionBudget
    spec:
      minAvailable: 1
      selector:
        matchLabels:
          dask.org/cluster-name: "" # Cluster name will be added automatically
          dask.org/component: scheduler

  worker-template-path: null

  scheduler-template:
    kind: Pod
    metadata:
    spec:
      restartPolicy: Never
      serviceAccountName: jupyter
      containers:
        - name: dask-scheduler
          # image: ghcr.io/dask/dask:latest
          image: hub.opensciencegrid.org/usatlas/analysis-dask-base:dev
          env:
            - name: DASK_BOKEH_PREFIX
              value: "{host}"
          args: ["dask", "scheduler", "--port", "8786", "--bokeh-port", "8787"]
          resources:
            limits:
              cpu: "2.0"
              memory: 20G
            requests:
              cpu: "1.5"
              memory: 10G

  worker-template:
    kind: Pod
    metadata:
    spec:
      restartPolicy: Never
      serviceAccountName: jupyter
      containers:
        - name: dask-worker
          image: hub.opensciencegrid.org/usatlas/analysis-dask-base:dev
          # image: ghcr.io/dask/dask:latest
          args:
            [
              "dask",
              "worker",
              "--nthreads",
              "1",
              "--memory-limit",
              "8GB",
              "--death-timeout",
              "60",
            ]
          resources:
            limits:
              cpu: "2"
              memory: 20G
            requests:
              cpu: "1"
              memory: 10G
